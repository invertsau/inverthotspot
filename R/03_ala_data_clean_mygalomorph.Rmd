---
title: "Data cleaning"
author: "Ashley Browse"
date: "2023-08-11"
output: 
    rmdformats::downcute:
    self_contained: true
    toc_depth: 3
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=TRUE)
```

<style>
#toc ul.nav li ul li {
    display: none;
    max-height: none;
}

#toc ul.nav li.active ul li  {
    display: block;
    max-height: none;
}

#toc ul.nav li ul li ul li {
    max-height: none;
    display: none !important;
}

#toc ul.nav li ul li.active ul li {
    max-height: none;
    display: block !important;
}
</style>



# Cleaning data from ALA

<!-- ## Load dependencies -->

```{r loadpackages, eval=TRUE, echo=FALSE}
# install.packages("pacman")
pacman::p_load(galah, arrow, here, tidyverse, janitor, dplyr, galah, worrms, ozmaps, sf, CoordinateCleaner, maps, job, zoomerjoin)

source(here("R/utils.R"))

```


<!-- Introduction message, and define variables  -->
```{r, echo=F}
focus_taxon <- "Mygalomorphae spider"
focus_taxon_short <- "Mygal"

```

This document lays out the workflow of cleaning **`r focus_taxon`** data from the ALA database using R.


<!-- ## Read in parquet (observations retrieved from ALA) -->

```{r, eval=TRUE, echo=FALSE}
myg_spiders_all <- open_dataset(here(get_latest_download()))
```

<!-- ## Subset columns we want -->

```{r, eval=TRUE, echo=FALSE}
myg_spiders <- myg_spiders_all |> 
  select(recordID:outlierLayer) |> 
  collect()

unique_species <- unique(myg_spiders$species) |> length()

```

The data includes **`r unique_species`** species of **`r focus_taxon_short`s**, with **`r length(myg_spiders$recordID)`** records in total. Some pre cleaning of the data was done using ALA before downloading, but many of these records may still have errors.

## Taxonomic errors

<!-- Clean for taxonomic errors: misspelt names, incorrect, synonyms.  -->

Here we clean the data for taxonomic errors, such as misspelt and outdated names.

We verified that all records have species level taxonomic data.
There were **`r length(myg_spiders$recordID) - count_before_filter`** records without species level data.

### Read in Australian Faunal Directory (AFD) data to cross check out taxa

The [Australian Faunal Directory (AFD)](https://biodiversity.org.au/afd/home) is an online catalogue of taxonomic and biological information on all animal species known to occur within Australia and its territories.

The AFD can provide information on: 

- nomenclature and taxonomy of species (including current, previous, and synonyms of species names);
- type data information;
- bibliographic information for species;
- distribution of species.

We used AFD data to ensure all species names in the ALA dataset are valid.

<!-- Check names against a naming authority to make sure all species included in the analyses are valid -->

```{r, echo=FALSE}
afd <- open_dataset(here("data/afd_05-2023_clean.parquet"))

# Filter down to Arachnida 
arachnids <- afd |> 
  filter(CLASS == "ARACHNIDA") |> 
  collect()
```

<!-- Where is Mygalomorphae in the AFD? -->
<!-- Filter down to Mygalomorphae using `HIGHER_CLASSIFICATION` -->
 
```{r, eval=FALSE, echo=F}
arachnids |> 
  pull(HIGHER_CLASSIFICATION) |> 
  str_subset(pattern = "MYGALOMORPH")
```

<!-- How many matches of ALA data do we have in the AFD? How many species did not match? Which species don't match? -->

```{r, echo=FALSE}
# how many species in ALA data matched species in the AFD?
AFD_matched <- myg_spiders |> 
  filter(scientificName %in% arachnids$FULL_NAME) |> 
  pull(scientificName) |> 
  unique() |> 
  length()  # put into sentence form

# How many and which ones are the species in ALA data that DID NOT match with the AFD
AFD_unmatched <- myg_spiders |> 
  filter(! scientificName %in% arachnids$FULL_NAME) |> 
  pull(scientificName) |> 
  unique() |> 
  length()

# Why are these species not in the AFD? These might be synonyms! 
unmatched_ala_sp <- myg_spiders |> 
  filter(! scientificName %in% arachnids$FULL_NAME) |> 
  pull(scientificName) |> 
  unique() 

```

Of the **`r unique_species`** unique **`r focus_taxon_short`** species in the ALA dataset, **`r AFD_matched`** matched those listed in the AFD (**`r round(AFD_matched/unique_species *100, 2)`** %). These **`r AFD_unmatched`** species did not match:
_**`r unmatched_ala_sp`**_.

<!-- Which of the ALA species not matched to AFD occur in the list of AFD arachnid species synonyms -->

The AFD lists any applicable synonyms for each species within their database. Our **`r AFD_unmatched`** missing species might be synonyms of listed species.

```{r, echo=F}
afd_synonyms <- word(arachnids$SYNONYMS, 1, 2)[complete.cases(word(arachnids$SYNONYMS, 1, 2))]

fuzzy_matched <- unmatched_ala_sp |> 
  jaccard_left_join(afd_synonyms, n_bands = 20, band_width = 6, threshold = .8) |> 
  rename(ALA_names = value.x,
         AFD_synonyms = value.y)

issynonym_count = is.na(fuzzy_matched$AFD_synonyms) |> table() |> _[1]

```

**r issynonym_count`** unmatched ALA species are potential AFD synonyms.

The synonym matches are as follows: 

```{r}
fuzzy_matched
```

These will be verified by experts to determine how to best handle them.

<!-- Excluding introduced/invasives taxa -->

### Use World Spider Catalog and GRIIS to exclude introduced spiders from dataset

The [World Spider Catalogue (WSC)](https://wsc.nmbe.ch/) is a comprehensive online database of spiders from around the world, with detailed taxonomic information, distribution maps, references and images.

```{r, echo=F, include=F}
# Read in spider data downloaded from WSC
wsc <- read_csv(here("data/wsc/species_export_20230929.csv"))
```


<!-- Have a look at distribution variable: How many contain "Australia"? Which ones are "Introduced to Australia"?  -->

```{r, echo=F}
# We want to capture all observations where "Introduced to" and "Australia" occurs together
# Example:
#str_subset(wsc$distribution, pattern = "Introduced to.*Australia")
#str_detect(wsc$distribution, pattern = "Introduced to.*Australia")
#str_which(wsc$distribution, pattern = "Introduced to.*Australia")

introduced_taxa <- wsc |> 
  filter(str_detect(distribution, pattern = "Introduced to.*Australia")) 

# We need to join genus and species together manually
introduced_taxa_wfullname <- introduced_taxa |> 
  mutate(scientific_name = paste(genus, species)) 

# introduced_taxa_wfullname |> 
#   select(scientific_name)

```

WSC lists **`r length(introduced_taxa$speciesId)`** species as introduced to Australia.

<!-- Cross match introduced taxa from WSC with ALA data to exclude introduced spiders -->

```{r, echo=F, include=F}
introduced_taxa_wfullname$scientific_name %in% myg_spiders$scientificName |> table()
myg_spiders$scientificName %in% introduced_taxa_wfullname$scientific_name |> table()
no_Introduced_WSC <- length(which(myg_spiders$scientificName %in% introduced_taxa_wfullname$scientific_name))
```

There are **`r no_Introduced_WSC`** introduced species of spiders in the ALA data according to the WSC data.


<!-- ### Global Register of Invasive and Introduced Species (GRIIS) -->

The [Global Register of Invasive and Introduced Species (GRIIS)](http://griis.org/) is a project by the IUCN SSC Invasive Species Specialist Group to compile annotated and verified country-wise inventories of introduced and invasive species.

```{r, echo=F, include=F}
# Join distrubtion.txt and taxon-edit.txt
griis_distrib <- read.delim(here("data/griis-australia-v1.6/distribution.txt"))
griis_taxon <- read.delim(here("data/griis-australia-v1.6/taxon-edited.txt"))
# check if both lists are the same length
count(griis_distrib) #length = 2984
count(griis_taxon) #length = 2960
# which IDs are missed? (What is in Distribution that is not in Taxon?)
setdiff(griis_distrib$id, griis_taxon$id) |> length() # 24 missing
# What's in Taxon that is not in Distribution?
setdiff(griis_taxon$id, griis_distrib$id) # none missing

griis_all <- left_join(griis_distrib, griis_taxon, by = "id")  # Apply left_join dplyr function 

# filter by class = arachnida
str_subset(griis_all$class, "Arachnida") # no results
# show unique values to double check there are no arachnids
griis_all$class |> unique() # there are no arachnids in this list

# Cross match with ALA data

# length(str_subset(griis_all$class, "Arachnida"))

```

The GRIIS contains **`r count(griis_all)`** records of introduced species in Australia. 

**`r length(str_subset(griis_all$class, "Arachnida"))`** of these are Arachnids.

### Subspecies

<!-- Decide whether to keep sub-species and if not whether to combine their data with the parent species -->
 
```{r, echo=F}
# look for subspecies
# what taxon levels are in the ALA data?
# myg_spiders$taxonRank |> unique() #what is "unranked" and "NA"
# myg_spiders |> filter(taxonRank == "unranked") #no species IDs
# myg_spiders |> filter(taxonRank == "NA") #nothing

# look at unique species
# myg_spiders$species |> unique() #no subspecies here

# check if any species have more than 2 words:
# Function to count words in a string
count_words <- function(string) {
  words <- strsplit(string, "\\s+")[[1]]  # Split the string into words
  return(length(words))                   # Return the number of words
}
word_counts <- lapply(myg_spiders$species |> unique(), count_words)
# which(word_counts != 2) 

# myg_spiders$species |> unique() |> str_count(pattern = "\\S+") > 2

```

There are **`r length(which(word_counts != 2))`** subspecies of `r focus_taxon_short`s within the ALA data.

### Excluding Marine Species

The World Register of Marine Species (WoRMS) provides a authoritative and comprehensive list of names of marine organisms, including currently valid and alternative names.

This function cross matches our species list to the WoRMS database

```{r, eval=FALSE, echo=F}
# wm_records_taxamatch("Teranodes")
myg_spiders <- myg_spiders$species |> unique()

# job::job({
#   output_df <- map(myg_spiders,
#                    possibly(~wm_records_taxamatch(.x, marine = FALSE)  |>
#                               pluck(1)  |>
#                               mutate(search_term = .x) |>
#                               discard(.p = ~is.null(.x))
#                    )
#   ) |>
#     list_rbind()
# 
#   saveRDS(output_df, "output/worrms_myg_wm_records_taxamatch.rds")
# })
```



```{r, echo=FALSE}
#read RDS

wormsOutput <- readRDS(here("output/worrms_myg_wm_records_taxamatch.rds"))
str(wormsOutput)
head(wormsOutput)
wormsOutput$match_type

#%in%
#find exact matches between our ALA data and worms output
length(which(wormsOutput$match_type %in% "exact"))
wormsOutput$match_type %in% "near"
wormsOutput$scientificname
wormsOutput$search_term
wormsOutput |> select(scientificname, search_term, match_type)
names(wormsOutput)

```

There are `r length(which(wormsOutput$match_type %in% "exact"))` matches

### Geographic errors

Here we clean the data for geographic errors.

<!-- Remove records not in Australia -->

```{r}
# show records not in Australia?
subset(myg_spiders, myg_spiders$country != "Australia")$country

# keep only those in Australia
myg_spiders <- subset(myg_spiders, myg_spiders$country == "Australia")
```

<!-- Plot data -->

```{r}
map_area <- borders(col = "grey80", 
          xlim = range(myg_spiders$decimalLongitude), 
          ylim = range(myg_spiders$decimalLatitude))
ggplot() +
  coord_fixed() + 
  map_area + 
  geom_point(data = myg_spiders,
             aes(x = decimalLongitude, y = decimalLatitude),
             colour = "black",
             size = 0.5) +
  theme_bw()

```


Coordinate Cleaner

https://docs.ropensci.org/CoordinateCleaner/

`.equ` = Equal longitude and latitude
`.sea` = XXX 

### Report how many records were removed under all Coordinate Cleaner tests

```{r}
# define tests list
all_tests <- c("capitals", "centroids", "duplicates", "equal", "gbif", "institutions", "outliers", "ranges", "seas", "urban", "zeros")
# run tests
flagged_alltests <- clean_coordinates(x = myg_spiders,
                                      "decimalLongitude","decimalLatitude", 
                                      tests = all_tests)

#how many flagged?
summary(flagged_alltests)
nrow(flagged_alltests) #total records
nrow(subset(flagged_alltests, .summary==TRUE)) #clean records
nrow(subset(flagged_alltests, .summary==FALSE)) #flagged records

names(flagged_alltests)
#How many were flagged with each test
flagsums <- data.frame(Test = all_tests, 
                       Flags = c(sum(flagged_alltests[".cap"] == FALSE),
                                 sum(flagged_alltests[".cen"] == FALSE),
                                 sum(flagged_alltests[".dpl"] == FALSE),
                                 sum(flagged_alltests[".equ"] == FALSE),
                                 sum(flagged_alltests[".gbf"] == FALSE),
                                 sum(flagged_alltests[".inst"] == FALSE),
                                 sum(flagged_alltests[".otl"] == FALSE),
                                 0, #ranges test does not run
                                 sum(flagged_alltests[".sea"] == FALSE),
                                 sum(flagged_alltests[".urb"] == FALSE),
                                 sum(flagged_alltests[".zer"] == FALSE)
                                 ))
flagsums
```

These are the tests performed and how many records were flagged in each

<!-- Also check AOHI -->
```{r}
#additional test
myg_spiders_aohi <- cc_aohi(myg_spiders)
#which were removed?
nrow(myg_spiders)
nrow(myg_spiders_aohi)

myg_spiders[which(myg_spiders$recordID == 
        setdiff(myg_spiders$recordID, myg_spiders_aohi$recordID)),]
```

<!-- Mapping all flagged records -->

```{r}
#default plot
#plot(flagged, lon = "decimalLongitude", lat = "decimalLatitude")

ggplot() +
  coord_fixed() + 
  map_area + 
  geom_point(data = myg_spiders,
             aes(x = decimalLongitude, y = decimalLatitude),
             colour = "green",
             size = 0.5) +
  geom_point(data = subset(flagged_alltests, .summary==FALSE),
             aes(x = decimalLongitude, y = decimalLatitude),
             colour = "darkred",
             size = 0.5) +
  theme_bw()
```


`nrow(subset(flagged, .summary==FALSE))` of `nrow(flagged)` records were flagged

<!-- define basemap for visualisation -->

```{r}
aus <- st_transform(ozmaps::ozmap_country, 4326)

base_map <- ggplot() + 
  geom_sf(data = aus, fill = NA) + 
  theme_minimal()
```


### Visualise .sea values

FALSE are values that are flagged to occur in sea and therefore will be eventually excluded


```{r, echo=F}
flagged_sf <- flagged_alltests |> 
  st_as_sf(coords = c("decimalLongitude", "decimalLatitude"), 
           crs = 4326) 

base_map  + 
  geom_sf(data = flagged_sf, aes(color = .sea), alpha = 0.5) +
  theme_minimal()
```

### Visualise .cap values

```{r, echo=F}
base_map  + 
  geom_sf(data = flagged_sf, aes(color = .cap), alpha = 0.5) + 
  theme_minimal()
```

### Visualise .otl values

```{r, echo=F}
base_map  + 
  geom_sf(data = flagged_sf, aes(color = .otl), alpha = 0.5) + 
  theme_minimal()
```

### Visualise .inst values

```{r, echo=F}
base_map  + 
  geom_sf(data = flagged_sf, aes(color = .inst), alpha = 0.5) + 
  theme_minimal()
```

### Exclude records flagged by CoordinateCleaner

```{r, echo=F}
myg_spiders_clean <- myg_spiders[which(flagged_alltests$.summary),]
```

## Deduplication
<!-- Check for duplicates (as the last step) -->

In this step we can inspect and clean the data of duplicates 

```{r, echo=F}
nrow(myg_spiders)
head(myg_spiders)

#investigating lat/long duplicates
myg_spiders_clean_coords <- myg_spiders_clean |> 
  filter(!is.na(decimalLatitude) & !is.na(decimalLongitude))

#checking for records with lat, long, and species duplicated
lat_long_species_dups <- duplicated(myg_spiders_clean_coords[,c(4:5,14)]) |
  duplicated(myg_spiders_clean_coords[,c(4:5,14)], fromLast = TRUE)

tabyl(lat_long_species_dups) # No records with lat, long, and species duplicated

# Deduplicating
deduplicated_myg_spiders <- myg_spiders_clean |> 
  filter(!is.na(decimalLatitude) & !is.na(decimalLongitude)) |> 
   group_by(scientificName, decimalLatitude, decimalLongitude, eventDate) |> 
   filter(n() == 1) |> 
  ungroup()
```

## Narrow down to species with enough observations

```{r}
# Filter down to species
myg_cleaned_sp <- myg_spiders_clean |> 
  filter(taxonRank == "species")

# Summarise number of observations by taxon
deduplicated_myg_spiders |> 
  group_by(scientificName) |> 
  summarise(n = n()) |> 
  arrange(-n)

# Which ones have less than or equal 3 
less_than_3_obs <- deduplicated_myg_spiders |> 
  group_by(scientificName) |> 
  summarise(n = n()) |> 
  filter(n <= 3) |> 
  pull(scientificName)

myg_sp_enoughobs <- deduplicated_myg_spiders |> 
  filter(!scientificName %in% less_than_3_obs)

# How many species
myg_sp_enoughobs |>
  dplyr::select(scientificName) |> 
  distinct() |> 
  nrow()
```

## Export cleaned data as a .parquet

<!-- Filename should contain the date of the parquet we read in at the start, export to output folder -->
```{r, echo=F}
file_name <- str_split_1(get_latest_download(), "/")[3] 

write_parquet(myg_sp_enoughobs, here(paste0("output/data/cleaned_", file_name)))
```

